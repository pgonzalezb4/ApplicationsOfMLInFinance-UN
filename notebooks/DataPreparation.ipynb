{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Data preparation\n",
    "#### Pablo Gonzalez B."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "68ccd6dd4fc8a282"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Variable Description\n",
    "\n",
    "`GRCODE NAIC` company code (including insurer groups and single insurers)\n",
    "\n",
    "`GRNAME NAIC` company name (including insurer groups and single insurers)\n",
    "\n",
    "`AccidentYear` Accident year(1988 to 1997)\n",
    "\n",
    "`DevelopmentYear` Development year (1988 to 1997)\n",
    "\n",
    "`DevelopmentLag` Development year (AY-1987 + DY-1987 - 1)\n",
    "\n",
    "`IncurLoss_` Incurred losses and allocated expenses reported at year end\n",
    "\n",
    "`CumPaidLoss_` Cumulative paid losses and allocated expenses at year end\n",
    "\n",
    "`BulkLoss_` Bulk and IBNR reserves on net losses and defense and cost containment expenses reported at year end\n",
    "\n",
    "`PostedReserve97_` Posted reserves in year 1997 taken from the Underwriting and Investment Exhibit – Part 2A, including net losses unpaid and unpaid loss adjustment expenses\n",
    "\n",
    "`EarnedPremDIR_` Premiums earned at incurral year - direct and assumed\n",
    "\n",
    "`EarnedPremCeded_` Premiums earned at incurral year - ceded\n",
    "\n",
    "`EarnedPremNet_` Premiums earned at incurral year - net\n",
    "\n",
    "`Single` 1 indicates a single entity, 0 indicates a group insurer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e84f3eeda967bf8b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Import libraries"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f54699ddd335f52"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy import stats"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3cdad7a7437c8e15"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Read data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3d8e14ceee9dd39b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data = pd.read_csv('medmal_pos.csv')\n",
    "data.shape"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e4fdc322a4c5705d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c3b4f80d5d5dd4f5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data preprocessing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bf0e33517072c747"
  },
  {
   "cell_type": "markdown",
   "source": [
    "En primera instancia, la variable `GRCODE` es una variable identificadora, por lo tanto no es una feature relevante para nuestro modelo, procedemos a eliminarla."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dc42ae1628c2fed9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data = data.drop(columns=['GRCODE'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "89b60937d6f8dbf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Anteriormente vimos en el EDA que hay una correlación fuerte entre las siguientes features:\n",
    "- `CumPaidLoss_F2`, `IncurLoss_F2`\n",
    "- `EarnedPremDIR_F2`, `IncurLoss_F2`\n",
    "- `EarnedPremDIR_F2`, `EarnedPremNet_F2`\n",
    "\n",
    "Las demás correlaciones son inducidas por estas anteriores, por lo tanto para evitar heterocedasticidad en nuestro modelo optaremos por eliminar las features agregadas como por ejemplo `CumPaidLoss_F2` y `EarnedPremDIR_F2`.\n",
    "\n",
    "Después de esto, quedamos con las siguientes correlaciones entre features:\n",
    "\n",
    "- `PostedReserve97_F2`, `IncurLoss_F2`\n",
    "- `PostedReserve97_F2`, `EarnedPremNet_F2`\n",
    "\n",
    "Si observamos la variable `PostedReserve97_F2`, esta variable es constante para distintos valores del tiempo (`AccidentYear` o `DevelopmentYear`), esto puede representar un obstáculo si decidimos modelar nuestro problema como un problema de forecasting de series de tiempo, por lo tanto la eliminamos."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "83d62918f173c119"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data = data.drop(columns=['CumPaidLoss_F2', 'EarnedPremDIR_F2', 'PostedReserve97_F2'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5b89d04701324c4b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ahora graficamos nuevamente nuestra matriz de correlaciones."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "27b1f56ce38c05b0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (22, 7)\n",
    "numeric_features = data.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "sns.heatmap(data[numeric_features].corr(method='spearman'), annot=True);"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9627bc543bc6ac63"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Solo nos queda una correlación por eliminar para evitar heterocedasticidad, aquella que se observa entre las features `EarnedPremNet_F2` y `IncurLoss_F2`. Si lo que deseamos es hacer un forecast de la pérdida, nuestras candidatas a variables objetivo serán `IncurLoss_F2` o `BulkLoss_F2`, por lo tanto no tiene sentido eliminar alguna de estas features. Por otro lado, tampoco eliminaremos la feature `EarnedPremNet_F2` porque si `IncurLoss_F2` llega a ser la variable objetivo, estaremos eliminando una feature que probablemente pueda explicar fuertemente la variación de `IncurLoss_F2`, por lo tanto dejaremos nuestro dataset así, y ahora procedemos a escalar y codificar nuestras features."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "127bc0c3b131f81f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "En nuestro EDA vimos que contamos con valores atípicos en algunas de nuestras features numéricas, por lo tanto procedemos a eliminar dichos valores, para eso utilizaremos el z-score de la distribución de cada feature y eliminaremos los valores que estén a cierta distancia del z-score."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "518bebd2c15ebab8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "processed_data = data[(np.abs(stats.zscore(data['IncurLoss_F2'])) < 3) & \\\n",
    "                      (np.abs(stats.zscore(data['BulkLoss_F2'])) < 3) & \\\n",
    "                      (np.abs(stats.zscore(data['EarnedPremCeded_F2'])) < 3) & \\\n",
    "                      (np.abs(stats.zscore(data['EarnedPremNet_F2'])) < 3)].copy().reset_index(drop=True)\n",
    "\n",
    "numeric_features = processed_data.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "non_numeric_features = processed_data.select_dtypes(include=['object']).columns.tolist()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e73db92aa9f09d3c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Posteriormente normalizamos nuestra variable textual para evitar textos duplicados por typos."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ec4dbc0a277c8158"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from cucco import Cucco\n",
    "\n",
    "cucco = Cucco()\n",
    "\n",
    "normalizations = [\n",
    "    'remove_extra_white_spaces',\n",
    "    ('replace_punctuation', {'replacement': ''})\n",
    "]\n",
    "\n",
    "# normalizamos la variable categorica GRNAME\n",
    "processed_data['GRNAME'] = processed_data['GRNAME'].apply(lambda x : cucco.normalize(x, normalizations))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "427ffdfdb5d0ee3b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "processed_data.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "78637824ecb2adeb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Después de haber normalizado la feature categorica de texto, aplicamos one-hot encoding a dicha feature categorica usando `pd.get_dummies()` y usando `StandardScaler()` de `sklearn` estandarizamos las features numéricas menos `AccidentYear`, `DevelopmentYear`, `DevelopmentLag` y `Single`, pues estandarizar estas features para nuestro modelo no tendría sentido, mucho menos las features de año."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a18e70b50fa5fc26"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "standard_scaler = StandardScaler()\n",
    "\n",
    "year_single_features = processed_data[['AccidentYear', 'DevelopmentYear', 'DevelopmentLag', 'Single']].copy()\n",
    "standardized = pd.DataFrame(standard_scaler.fit_transform(processed_data[numeric_features].copy().drop(columns=['AccidentYear', 'DevelopmentYear', 'DevelopmentLag', 'Single'])),\n",
    "                 columns=processed_data[numeric_features].copy().drop(columns=['AccidentYear', 'DevelopmentYear', 'DevelopmentLag', 'Single']).columns)\n",
    "one_hot_encoded = pd.get_dummies(processed_data[non_numeric_features].copy(), dtype=bool).replace({True : 1, False : 0})\n",
    "\n",
    "processed_data_scaled = pd.concat([year_single_features, standardized, one_hot_encoded], axis=1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "276aff5ef91c722b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "processed_data_scaled.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a18e1b78cc64fb82"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Verifiquemos que las features quedaron estandarizadas calculando su media y su desviación estándar."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8b3070fb7d1936be"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "standardized_features = ['IncurLoss_F2', 'BulkLoss_F2', 'EarnedPremCeded_F2', 'EarnedPremNet_F2']\n",
    "processed_data_scaled[standardized_features].describe().T[['mean', 'std']].round()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "634740828abba1e1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "En esta instancia hemos eliminado correlaciones entre nuestas features que puedan causar heterocedasticidad, imputamos valores atípicos que pueden darle problemas a nuestro modelo, codificamos nuestras features categoricas y estandarizamos nuestras features numericas, por lo tanto nuestros datos están listos para que un modelo sea ajustado."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3b8dcc54f03be70c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
