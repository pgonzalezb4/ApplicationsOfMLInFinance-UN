{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificador y generador de dígitos utilizando naive bayes\n",
    "### Pablo Gonzalez Baron"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importar librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib import pyplot\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importar datos MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "pyplot.subplot(331)\n",
    "pyplot.imshow(x_train[0], cmap=pyplot.get_cmap('gray'))\n",
    "pyplot.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformar datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(60000,784)\n",
    "x_test = x_test.reshape(10000,784)\n",
    "data = np.zeros([70000,784])\n",
    "data[:60000,:] = x_train; data[60000:,:] = x_test\n",
    "label = np.zeros(70000)\n",
    "label[:60000] = y_train; label[60000:] = y_test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construcción del modelo Naive Bayes\n",
    "A continuación se construye la función del modelo Naive Bayes que entrena el modelo con los datos de entrenamiento, lo evalua con los datos de test y luego genera una matriz de confusión, junto a la tasa de error de clasificación de cada digito individual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes(data,label):\n",
    "    n_s,n_f = data.shape          # Obtenemos el shape (sample, features) de nuestros datos\n",
    "    classes = np.unique(label)    # Obtenemos el nombre de las clases en las etiquetas de nuestros datos\n",
    "    n_c = len(classes)            # Numero de clases\n",
    "    total_data = np.zeros([n_s,n_f+1]) # Creamos una matriz de ceros con dimensiones (samples, feature+1) para guardar, como lo dicen las dimensiones, los samples y las features.\n",
    "    total_data[:,:-1] = data\n",
    "    total_data[:,-1] = label           # Guardamos las etiquetas en esta matriz previamente creada\n",
    "    np.random.shuffle(total_data)\n",
    "    X_train = total_data[:60000,:]\n",
    "    np.random.shuffle(X_train)\n",
    "    X_test = total_data[60000:,:]\n",
    "    np.random.shuffle(X_test)\n",
    "    X_test_c = X_test[:,:-1]             # Obtener samples y feature de la data de test\n",
    "    X_test_l = X_test[:,-1]              # Obtener labels de la data de test\n",
    "    mean_v = np.zeros([n_c,n_f])       # Crear una matriz de ceros que será usada para guardar la media de las features y clases\n",
    "    var_v = np.zeros([n_c,n_f])        # Crear una matriz de ceros que será usada para guardar la varianza de las features y clases\n",
    "    c_prob = []                        # Probabilidades del modelo\n",
    "    confusion_matrix = np.zeros([n_c,n_c]) # Crear matriz de confusion de tamaño (classes*classes)\n",
    "    d_acc = []                         # Accuracy de clasificación de cada dígito\n",
    "    \n",
    "    for c in classes:\n",
    "        X_train_c = X_train[X_train[:,-1] == c]\n",
    "        X_train_c = X_train_c[:,:-1]\n",
    "        c_prob.append(len(X_train_c)/len(X_train))\n",
    "        mean_v[int(c),:] = X_train_c.mean(axis=0)\n",
    "        var_v[int(c),:] = X_train_c.var(axis=0)\n",
    "    \n",
    "    var_v = var_v + 1000\n",
    "    count = 0             \n",
    "    \n",
    "    for i in range(X_test.shape[0]):\n",
    "        lists=[]   # Lista para almacenar la probabilidad de todas las clases para la observación i.\n",
    "        for j in range(n_c):\n",
    "            numerator = np.exp(-((X_test_c[i]-mean_v[j])**2)/(2*var_v[j])) \n",
    "            denominator = np.sqrt(2*np.pi*(var_v[j]))\n",
    "            ratio=np.sum(np.log(numerator/denominator)) # Probabilidad de que la feature i sea de la clase j\n",
    "            lists.append(ratio) # Añadir probabilidad de que la feature i sea de la clase j\n",
    "        \n",
    "        pred = lists.index(max(lists)) # Tomar el y predicho para la clase que tiene el valor maximo de probabilidad en el vector de predicción\n",
    "        if pred == X_test_l[i]: \n",
    "            count = count+1 # Si el valor predicho es igual al valor real, se incrementa el conteo\n",
    "            confusion_matrix[int(X_test_l[i])][int(X_test_l[i])] = confusion_matrix[int(X_test_l[i])][int(X_test_l[i])]+1\n",
    "            # Se añaden los valores correspondientes la matriz de confusión\n",
    "        else:\n",
    "            for k in range(n_c):\n",
    "                if pred == k:\n",
    "                    confusion_matrix[int(X_test_l[k])][int(X_test_l[i])] = confusion_matrix[int(X_test_l[k])][int(X_test_l[i])]+1\n",
    "                    # Se añaden los valores correspondientes la matriz de confusión\n",
    "    for l in classes:\n",
    "        check = X_test[X_test[:,-1] == l]\n",
    "        a = (confusion_matrix[int(l)][int(l)])/check.shape[0] # Calcular el accuracy de cada digito\n",
    "        d_acc.append(a)   # Añadir accuracy individual de cada digito\n",
    "    \n",
    "\n",
    "    o_acc = count/X_test.shape[0]\n",
    "    return(d_acc, o_acc, confusion_matrix, mean_v, var_v)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicciones con el modelo Naive Bayes\n",
    "\n",
    "A continuación vamos a usar la función que creamos anteriormente para ajustar nuestro modelo Naive Bayes a nuestro conjunto de entrenamiento, para luego evaluar nuestro modelo usando nuestro conjunto de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digit_accuracy, overall_accuracy, matrix, mean_v, var_v = naive_bayes(data,label)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graficar la media y la varianza de cada dígito"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean of each digit in 28*28 form\n",
    "plt.figure(figsize=(16,8))\n",
    "for i in range(mean_v.shape[0]):\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    img = mean_v[i].reshape(28,28)\n",
    "    plt.imshow(img, cmap=\"Greys\")\n",
    "    plt.xlabel('Image of digit '+str(i)+' Mean',fontsize = 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variance of each digit in 28*28 form\n",
    "plt.figure(figsize=(16,8))\n",
    "for i in range(var_v.shape[0]):\n",
    "    plt.subplot(2,5, i+1)\n",
    "    img = var_v[i].reshape(28,28)\n",
    "    plt.imshow(img, cmap=\"Greys\")\n",
    "    plt.xlabel('Image of digit '+str(i)+' Variance',fontsize = 12)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Desempeño de generalizacion del modelo\n",
    "En esta ocasión utilizamos el accuracy de clasificación de cada dígito y la matriz de confusión para medir la capacidad del modelo para generalizar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digit = np.unique(label).astype('int64')\n",
    "naive_df = pd.DataFrame(list(zip(digit, digit_accuracy)), columns = ['Digito','Accuracy'])\n",
    "naive_df.set_index('Digito', inplace=True)\n",
    "print('Accuracy de clasificación para cada dígito:')\n",
    "naive_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,7))\n",
    "sns.heatmap(matrix, annot=True, cmap=\"YlGnBu\")\n",
    "plt.xlabel('True class value')\n",
    "plt.ylabel('Predicted class value');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ecusforecasting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
